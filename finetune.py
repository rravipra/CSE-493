# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hTGYpeLkoRX2RGDh8i3H8Cou9F7zg_PZ
"""

# install transformers
!pip install transformers

# Import required libraries as of now (use pytorch mainly)

import torch
from transformers import BertTokenizer, BertForMaskedLM
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("/content/drive/MyDrive/philosophy_data.csv")
df

texts = list(df['sentence_str'])[:1000]

# Load tokenizer and pre-trained BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# Split train_texts into training and test sets
train_texts, test_texts = train_test_split(texts, test_size=0.2, random_state=42)

# Tokenize and convert training data
train_inputs = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')

# Tokenize and convert test data
test_inputs = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')

train_inputs

# Fine-tune BERT model using adam optimizer

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
loss = torch.nn.CrossEntropyLoss()
model.train()

# Use GPU if available
model.to('cuda')

train_inputs = train_inputs.to('cuda')
train_labels = train_inputs['input_ids'].clone()
train_labels[train_labels == tokenizer.pad_token_id] = -100

optimizer.zero_grad()
outputs = model(**train_inputs, labels=train_labels)
loss = outputs.loss
loss.backward()
optimizer.step()

# Save the fine-tuned model
model.save_pretrained('fine_tuned_bert')

# Load the saved fine-tuned model
loaded_model = BertForMaskedLM.from_pretrained('fine_tuned_bert')
loaded_model.to('cuda')  # Use GPU if available

# Compute perplexity on test dataset
test_inputs = test_inputs.to('cuda')
test_labels = test_inputs['input_ids'].clone()
test_labels[test_labels == tokenizer.pad_token_id] = -100

loaded_model.eval()
with torch.no_grad():
    outputs = loaded_model(**test_inputs, labels=test_labels)

test_loss = outputs.loss.item()

perplexity = 2 ** test_loss

print(f"Perplexity on the test set: {perplexity}")